{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3877739a-8504-4cf7-a416-d8affb08845d",
   "metadata": {},
   "source": [
    "# Finetuning a llama2 on a new programming languange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9395b2-5831-423a-bf08-895a6a652e26",
   "metadata": {},
   "source": [
    "In this notebook, we will teach llama2 an old programming language that it has no knowledge of. OPL is a programming language from the 1980's created Psion in the UK. It powered their Psion Organisers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165d25a-7d18-482d-a01b-b7517a285a42",
   "metadata": {},
   "source": [
    "This Notebook is designed to run on a T4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c640c-1917-4800-80dd-82f59ff30eef",
   "metadata": {},
   "source": [
    "## Setup the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aba4be-71cb-4c5e-bf82-f7723e6c0a9c",
   "metadata": {},
   "source": [
    "The following section performs all the setup of the model. This includes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ea74a-e73b-4a7d-92a8-6cbc35e79ad9",
   "metadata": {},
   "source": [
    "- Installing any dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c2d6b-fffd-4777-bedc-bd43160159e9",
   "metadata": {},
   "source": [
    "- Setting any configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796580de-7d8b-46ec-8e39-62994cb6f916",
   "metadata": {},
   "source": [
    "- Downloading the Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d542c-bf6d-48ef-a17a-fea8524ef8f6",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d677db-f51a-448c-a92c-19f721501295",
   "metadata": {},
   "source": [
    "In order to get started we need to install the appropriate dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f340b6b8-b211-4ba4-9287-9370373155a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install trl for the SFT library\n",
    "!pip install -q trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c2df57-6a10-4fcf-b56e-ad55e0e8a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install bitsandbytes for quantization\n",
    "!pip install -q bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffa9846-7fad-459c-9daf-d6ea20c7a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages (0.1.99)\r\n"
     ]
    }
   ],
   "source": [
    "# we need sentencepiece for the llama2 slow tokenizer\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c661c653-1943-45f5-abff-4e87b92203e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need einops, used by falcon-7b, llama-2 etc\n",
    "# einops (einsteinops) is used to simplify tensorops by making them readable\n",
    "!pip install -q -U einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1428d5fc-25ab-47b8-8b76-322a63ae3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to install datasets for our training dataset\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312095c5-5ce4-4260-9c19-7e9093821750",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a70437-ec45-4117-bf0e-720a9898464d",
   "metadata": {},
   "source": [
    "The following configures our settings for finetuning our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43b9f15d-bd13-475a-85dd-58a4b168f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a1bf40-78f1-4c2f-8e03-c3292e65fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The instruction dataset to use\n",
    "dataset_name = \"chrishayuk/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e551d192-0c5e-4657-a8c0-ac48abeb0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-chuk-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab1ccda-52a1-4a50-b8e9-715c562dbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9198e796-0851-4c7b-b379-cd310050d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_train_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb49228-ada1-42dc-b811-d11a55645d35",
   "metadata": {},
   "source": [
    "### Download the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba07dce-debb-4ffe-982b-629722cbbf92",
   "metadata": {},
   "source": [
    "The following will download the base model, in this case the llama-2-7b-chat-hf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9defc9fb-004c-4573-b72d-20769bf78096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3feca2ef-a4ac-4bb2-a9f6-bfe8865473d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44667d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6d198b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50fd7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from the model (llama2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1aea7",
   "metadata": {},
   "source": [
    "### Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758be64",
   "metadata": {},
   "source": [
    "The following tests the capabilities of the language model prior to fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5391b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81403968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Write a hello world program in the OPL programming language.  [/INST]  The OPL (Optimizing Programming Language) is a programming language that was developed in the 1970s and 1980s for use in high-performance computing applications. Unfortunately, the language is no longer widely used or supported, and there are few resources available for learning or programming in OPL.\n",
      "\n",
      "As such, I must inform you that it is not possible to write a \"hello world\" program in OPL as there are no existing resources or tools available to support the language. The language is considered obsolete and is no longer supported by any major software vendors or communities.\n",
      "\n",
      "I apologize for any disappointment this may cause, but I hope you understand that it is not possible to learn or use an obsolete programming language without access to the necessary resources and support. There are many other programming languages that are\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "#prompt = \"What does the REM keyword stand for in OPL?\"\n",
    "#prompt = \"What was the first implementation of the OPL programming language?\"\n",
    "#prompt = \"Who were involved in the creation of OPL for Psion?\"\n",
    "#prompt = \"Did Colly Myers create OPL?\"\n",
    "prompt = \"Write a hello world program in the OPL programming language. \"\n",
    "#prompt = \"Write a limerick as comments in the OPL programming language. \"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e34b1",
   "metadata": {},
   "source": [
    "### Finetune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3cf21",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71682d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "125b70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1399f6",
   "metadata": {},
   "source": [
    "#### Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d02c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e230fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de44083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "501abf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d361d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f9a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0c66871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,      # uses the number of epochs earlier\n",
    "    per_device_train_batch_size=4,          # 4 seems reasonable\n",
    "    gradient_accumulation_steps=2,          # 2 is fine, as we're a small batch\n",
    "    optim=\"paged_adamw_32bit\",              # default optimizer\n",
    "    save_steps=0,                           # we're not gonna save\n",
    "    logging_steps=10,                       # same value as used by Meta\n",
    "    learning_rate=2e-4,                     # standard learning rate\n",
    "    weight_decay=0.001,                     # standard weight decay 0.001\n",
    "    fp16=False,                             # set to true for A100\n",
    "    bf16=False,                             # set to true for A100\n",
    "    max_grad_norm=0.3,                      # standard setting\n",
    "    max_steps=-1,                           # needs to be -1, otherwise overrides epochs\n",
    "    warmup_ratio=0.03,                      # standard warmup ratio\n",
    "    group_by_length=True,                   # speeds up the training\n",
    "    lr_scheduler_type=\"cosine\",           # constant seems better than cosine\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83e2b416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_42046/2106895844.py\", line 2, in <module>\n",
      "    trainer = SFTTrainer(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/trl/trainer/sft_trainer.py\", line 197, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/transformers/trainer.py\", line 541, in __init__\n",
      "    self.callback_handler = CallbackHandler(\n",
      "                            ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/transformers/trainer_callback.py\", line 305, in __init__\n",
      "    logger.warning(\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/transformers/trainer_callback.py\", line 322, in add_callback\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/transformers/integrations.py\", line 585, in __init__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,                # use our lora peft config\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,                    # no max sequence length\n",
    "    tokenizer=tokenizer,                    # use the llama tokenizer\n",
    "    args=training_arguments,                # use the training arguments\n",
    "    packing=False,                          # don't need packing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0284734d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save trained model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(new_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc9ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
