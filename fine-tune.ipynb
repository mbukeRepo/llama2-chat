{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3877739a-8504-4cf7-a416-d8affb08845d",
   "metadata": {},
   "source": [
    "# Finetuning a llama2 on a new programming languange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9395b2-5831-423a-bf08-895a6a652e26",
   "metadata": {},
   "source": [
    "In this notebook, we will teach llama2 an old programming language that it has no knowledge of. OPL is a programming language from the 1980's created Psion in the UK. It powered their Psion Organisers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165d25a-7d18-482d-a01b-b7517a285a42",
   "metadata": {},
   "source": [
    "This Notebook is designed to run on a T4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c640c-1917-4800-80dd-82f59ff30eef",
   "metadata": {},
   "source": [
    "## Setup the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aba4be-71cb-4c5e-bf82-f7723e6c0a9c",
   "metadata": {},
   "source": [
    "The following section performs all the setup of the model. This includes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ea74a-e73b-4a7d-92a8-6cbc35e79ad9",
   "metadata": {},
   "source": [
    "- Installing any dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c2d6b-fffd-4777-bedc-bd43160159e9",
   "metadata": {},
   "source": [
    "- Setting any configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796580de-7d8b-46ec-8e39-62994cb6f916",
   "metadata": {},
   "source": [
    "- Downloading the Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d542c-bf6d-48ef-a17a-fea8524ef8f6",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d677db-f51a-448c-a92c-19f721501295",
   "metadata": {},
   "source": [
    "In order to get started we need to install the appropriate dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f340b6b8-b211-4ba4-9287-9370373155a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install trl for the SFT library\n",
    "!pip install -q trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c2df57-6a10-4fcf-b56e-ad55e0e8a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install bitsandbytes for quantization\n",
    "!pip install -q bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffa9846-7fad-459c-9daf-d6ea20c7a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages (0.1.99)\r\n"
     ]
    }
   ],
   "source": [
    "# we need sentencepiece for the llama2 slow tokenizer\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c661c653-1943-45f5-abff-4e87b92203e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need einops, used by falcon-7b, llama-2 etc\n",
    "# einops (einsteinops) is used to simplify tensorops by making them readable\n",
    "!pip install -q -U einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1428d5fc-25ab-47b8-8b76-322a63ae3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to install datasets for our training dataset\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312095c5-5ce4-4260-9c19-7e9093821750",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a70437-ec45-4117-bf0e-720a9898464d",
   "metadata": {},
   "source": [
    "The following configures our settings for finetuning our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43b9f15d-bd13-475a-85dd-58a4b168f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a1bf40-78f1-4c2f-8e03-c3292e65fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The instruction dataset to use\n",
    "dataset_name = \"chrishayuk/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e551d192-0c5e-4657-a8c0-ac48abeb0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-chuk-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab1ccda-52a1-4a50-b8e9-715c562dbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9198e796-0851-4c7b-b379-cd310050d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_train_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb49228-ada1-42dc-b811-d11a55645d35",
   "metadata": {},
   "source": [
    "### Download the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba07dce-debb-4ffe-982b-629722cbbf92",
   "metadata": {},
   "source": [
    "The following will download the base model, in this case the llama-2-7b-chat-hf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9defc9fb-004c-4573-b72d-20769bf78096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3feca2ef-a4ac-4bb2-a9f6-bfe8865473d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44667d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6d198b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50fd7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from the model (llama2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1aea7",
   "metadata": {},
   "source": [
    "### Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758be64",
   "metadata": {},
   "source": [
    "The following tests the capabilities of the language model prior to fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5391b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81403968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Write a hello world program in the OPL programming language.  [/INST]  The OPL (Optimizing Programming Language) is a programming language that was developed in the 1970s and 1980s for use in high-performance computing applications. Unfortunately, the language is no longer widely used or supported, and there are few resources available for learning or programming in OPL.\n",
      "\n",
      "As such, I must inform you that it is not possible to write a \"hello world\" program in OPL as there are no existing resources or tools available to support the language. The language is considered obsolete and is no longer supported by any major software vendors or communities.\n",
      "\n",
      "I apologize for any disappointment this may cause, but I hope you understand that it is not possible to learn or use an obsolete programming language without access to the necessary resources and support. There are many other programming languages that are\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "#prompt = \"What does the REM keyword stand for in OPL?\"\n",
    "#prompt = \"What was the first implementation of the OPL programming language?\"\n",
    "#prompt = \"Who were involved in the creation of OPL for Psion?\"\n",
    "#prompt = \"Did Colly Myers create OPL?\"\n",
    "prompt = \"Write a hello world program in the OPL programming language. \"\n",
    "#prompt = \"Write a limerick as comments in the OPL programming language. \"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e34b1",
   "metadata": {},
   "source": [
    "### Finetune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3cf21",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71682d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "125b70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1399f6",
   "metadata": {},
   "source": [
    "#### Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d02c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e230fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de44083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "501abf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d361d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0c66871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,      # uses the number of epochs earlier\n",
    "    per_device_train_batch_size=4,          # 4 seems reasonable\n",
    "    gradient_accumulation_steps=2,          # 2 is fine, as we're a small batch\n",
    "    optim=\"paged_adamw_32bit\",              # default optimizer\n",
    "    save_steps=0,                           # we're not gonna save\n",
    "    logging_steps=10,                       # same value as used by Meta\n",
    "    learning_rate=2e-4,                     # standard learning rate\n",
    "    weight_decay=0.001,                     # standard weight decay 0.001\n",
    "    fp16=False,                             # set to true for A100\n",
    "    bf16=False,                             # set to true for A100\n",
    "    max_grad_norm=0.3,                      # standard setting\n",
    "    max_steps=-1,                           # needs to be -1, otherwise overrides epochs\n",
    "    warmup_ratio=0.03,                      # standard warmup ratio\n",
    "    group_by_length=True,                   # speeds up the training\n",
    "    lr_scheduler_type=\"cosine\",           # constant seems better than cosine\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83e2b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,                \n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,                    \n",
    "    tokenizer=tokenizer,                    \n",
    "    args=training_arguments,                \n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0284734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2457, 'learning_rate': 0.00019744105246469263, 'epoch': 1.82}\n",
      "{'loss': 2.3364, 'learning_rate': 0.00018522168236559695, 'epoch': 3.64}\n",
      "{'loss': 1.6345, 'learning_rate': 0.000164140821963114, 'epoch': 5.45}\n",
      "{'loss': 1.2569, 'learning_rate': 0.00013639049369634876, 'epoch': 7.27}\n",
      "{'loss': 0.9618, 'learning_rate': 0.00010485622221144484, 'epoch': 9.09}\n",
      "{'loss': 0.7234, 'learning_rate': 7.281699277636572e-05, 'epoch': 10.91}\n",
      "{'loss': 0.5881, 'learning_rate': 4.360429701490934e-05, 'epoch': 12.73}\n",
      "{'loss': 0.4927, 'learning_rate': 2.025571894372794e-05, 'epoch': 14.55}\n",
      "{'loss': 0.4684, 'learning_rate': 5.199082004372957e-06, 'epoch': 16.36}\n",
      "{'loss': 0.4636, 'learning_rate': 0.0, 'epoch': 18.18}\n",
      "{'train_runtime': 147.9738, 'train_samples_per_second': 5.812, 'train_steps_per_second': 0.676, 'train_loss': 1.2171390438079834, 'epoch': 18.18}\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb7d69",
   "metadata": {},
   "source": [
    "#### Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a97888",
   "metadata": {},
   "source": [
    "The following runs the model post fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0c57de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Hello Chris program in psion opl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1f7f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bce79e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/llama-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "result = pipe(f\"[INST] {prompt} [/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8b4a0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Write a Hello Chris program in psion opl [/INST] PROC main:\n",
      " everybody:LOADELL \n",
      "\tPRINT \"Hello World\"\n",
      "\tGET\n",
      "ENDP \n",
      "\n",
      "PROC hello(a):LOADELL\n",
      "\tPRINT \"Hello \", a\n",
      "\tGET\n",
      "ENDP \n",
      "\n",
      "hello Chris:INT\n",
      "\n",
      "main:\n",
      "\thello Chris\n",
      "\tGET\n",
      "ENDP \n",
      "\n",
      "ENDP \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fef594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
